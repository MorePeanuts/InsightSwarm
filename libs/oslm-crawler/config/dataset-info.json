{
    "OpenGVLab/MMPR-v1.1": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/V2PE-Data": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "internlm/SWE-Fixer-Train-110K": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "internlm/SWE-Fixer-Eval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "facebook/2M-Belebele": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/2M-Flores-ASL": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/ExploreToM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/LCFO": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "facebook/Y-NQ": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/LongBench-v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "THUDM/MotionBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Qwen/CodeElo": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/FACTS-grounding-public": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/erased-cifar10": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/erased-cifarnet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_250000_uniform_r17": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_24000_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_200000_uniform_r17": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_6200_uniform_r17": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_127000_uniform_r17": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_15500_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_13200_uniform_r17": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_20000_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_41000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_19000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Aquila-135M-Datasets": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "alibabasglab/LJSpeech-1.1-48kHz": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/cc": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/madlad-400_vi": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/wikihow_vi-en-zh": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/KB_wikimedia": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/cc_vi_truyen": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/KB_tve-selected-books": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/RAG_UltraDomain": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/hplt-vi": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/c4_vi-filtered_200GB": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Symato/oscar-2301_vi": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/bge-reranker-data__vien": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Symato/goods_vs_c4_cc_classifiers": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Symato/bge-m3-data__vien": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Symato/fineweb_edu_10bt_shuffled": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Symato/NAM-005_436G_Vi-En-Code": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Symato/DOT_excite_data_v0.0": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/objaverse": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/c4": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/ai2_arc": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/MADLAD-400": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/winogrande": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/openbookqa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/ZebraLogicBench-private": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/social_i_qa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/dolmino-mix-1124": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/math_qa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/blog-images": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/sciq": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/reward-bench-results": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/s2-naip": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/olmo-mix-1124": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/reward-bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/OLMoE-mix-0924": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-sft-mixture": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/scirepeval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/prosocial-dialog": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/lila": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/wildguardmix": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/pixmo-docs": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/WildChat-1M": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/WildBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/objaverse-xl": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/peS2o": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/RLVR-GSM-MATH-IF-Mixed-Constraints": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/WildBench-V2-Model-Outputs": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/metaicl-data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/qasper": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/nllb": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/scirepeval_test": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/pixmo-cap": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/wildjailbreak": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/real-toxicity-prompts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/swag": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/scitail": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-v2-sft-mixture": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/qasc": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/preference-test-sets": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/dolma": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multi_lexsum": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/wmt22_african": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/break_data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/cosmos_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/pixmo-points": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/preference-datasets-tulu": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/ultrafeedback_binarized_cleaned": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-v3.1-mix-preview-4096-OLMoE": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-math": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/WildChat": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/xstest-response": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/quac": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/paloma": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/SciRIFF": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-math-grade": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/ACE2-ERA5-sample-output": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-algebra": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/mslr2022": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/layout_distribution_shift": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/llama-3.1-tulu-3-8b-preference-mixture": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/common_gen": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/pixmo-count": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/scitldr": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/pixmo-cap-qa": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-instruction-following": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-olmo-2-mixture": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/pixmo-ask-model-anything": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-2.5-preference-data": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/quartz": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/llama-3.1-tulu-3-70b-preference-mixture": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/pixmo-point-explanations": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/art": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/social_bias_frames": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/cord19": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/ropes": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/href": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/mathfish-tasks": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/coconot": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/scicite": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-code": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/sdsd-dialogues": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/wiqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/dolma-pes2o-cc-pd": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/SimpleToM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/pixmo-clocks": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/href_results": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/atomic": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/compred": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/RLVR-GSM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ZebraLogicBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/RLVR-IFeval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ContextEval": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/mathfish": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/RLVR-MATH": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/discoverybench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/scifact": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/scifact_entailment": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/soda": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/peer_read": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-wildchat-unused": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/csabstruct": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multipref": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/scico": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/pixmo-points-eval": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/satlas-super-resolution": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/zest": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/quoref": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/sdsd-revisions": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/commongen_lite": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-wildchat-ultrafeedback": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-pref-personas-instruction-following": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/olmo-2-1124-13b-preference-mix": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/super": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/multinews_sparse_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/DS_Critique_Bank": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/gooaq": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/drug-combo-extraction": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tulu-3-hardcoded-preferences": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/dolma-cccc": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-v2-sft-mixture-olmo-4096": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/wcep_sparse_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/wcep_dense_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-reused-off-policy": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-sft-prompts-ultrafeedback": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-v2-sft-mixture-olmo-2048": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multixscience_dense_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/cochrane_dense_mean": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-ultrafeedback-cleaned-on-policy-8b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tuple_ie": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multinews_dense_oracle": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-sft-reused-on-policy-8b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-wildchat-if-on-policy-70b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/pixmo-pointing": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/multinews_sparse_max": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/multixscience_dense_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/multinews_sparse_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/olmo-2-1124-7b-preference-mix": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-IF-augmented-on-policy-70b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/multixscience_sparse_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/multixscience_sparse_max": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multixscience_dense_max": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/ultrafeedback_binarized_cleaned_train": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/object-edit": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tulu-3-IF-augmented-on-policy-8b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-hard-coded-10x": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tulu-v1-sft-mixture": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ms2_dense_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/cochrane_dense_oracle": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-sft-reused-on-policy-70b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/achieve-the-core": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/multinews_dense_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-wildchat-if-on-policy-8b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-wildchat-reused-on-policy-70b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/hippocorpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multixscience_sparse_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/href_preference": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/wcep_dense_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ms2_sparse_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ms2_dense_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/mup": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/olmo-2-hard-coded": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-wildchat-reused-on-policy-8b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/WildChat-nontoxic": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/persona-bias": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/scrapinghub-article-extraction-benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/tulu-3-hardcoded-prompts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/SciRIFF-train-mix": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ms2_sparse_mean": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/multinews_dense_max": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-3-ultrafeedback-cleaned-on-policy-70b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/wcep_sparse_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/ms2_sparse_max": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/wcep_dense_max": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/reward-bench-cleaned-preview": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/UNcommonsense": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/points": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/ms2_dense_max": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/sci-sentences-10k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/aboutme": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/tulu-v2-sft-long-mixture": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/cochrane_sparse_mean": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/wcep_sparse_max": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/cochrane_sparse_oracle": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/mup-full": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/llama-3-tulu-v2-sft-subset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/openassistant-guanaco-reformatted": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/commongen_lite_eval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/fos_model_training_data_open_ai_annotations": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/cochrane_dense_max": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/cochrane_sparse_max": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-2.5-prompts": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/olmoe-dclm-metrics": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/analysis_olmoe": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/clocks": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/basic_arithmetic": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/analysis_mixtral": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/dolma_tagger_analysis": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "lavita/medical-qa-shared-task-v1-toy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/ChatDoctor-HealthCareMagic-100k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/medical-qa-datasets": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/MedQuAD": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/AlpaCare-MedInstruct-52k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/ChatDoctor-iCliniq": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/medical-eval-sphere": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/imapScore": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/medical-qa-shared-task-v1-toy-eval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/medical-qa-shared-task-v1-half": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/MedREQAL": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lavita/medical-qa-shared-task-v1-all": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceFW/fineweb": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceFW/fineweb-edu": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceFW/fineweb-2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceFW/admin": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceFW/fineweb-edu-score-2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceFW/fineweb-edu-llama3-annotations": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-PDF-CC-2024-10": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/datacomp_xlarge": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/dclm-baseline-1.0-parquet": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-HTML": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/dclm-baseline-1.0": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/datacomp_pools": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-PDF-CC-2023-06": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-PDF-CC-2023-50": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-PDF-CC-2023-40": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-PDF-CC-2023-23": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-PDF-CC-2023-14": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/dclm-pool-7b-2x": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/dclm-pool-1b-5x": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/datacomp_1b": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/dclm-pool-1b-1x": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/dclm-pool-7b-1x": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/MINT-1T-PDF-CC-2024-18": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/datacomp_large": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/dclm-pool-400m-1x": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/DataComp-12M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/MINT-1T-ArXiv": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/VisIT-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mlfoundations/tabula-8b-eval-suite": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/datacomp_small": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/datacomp_medium": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/refinedweb_banned_domains_curated": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/paloma_validation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/t4-full": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mlfoundations/open_lm_example_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/downstream_validation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/c4_validation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/downstream_validation_qa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/open_lm_test_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/open_lm_test_data_v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/datacomp_indices": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "AquaV/genshin-voices-separated": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/fallout-4-voices": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/mil-docs": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/Multi-Environment-Operations-Sharegpt": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/Interrogation-Sharegpt": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/US-Army-Survival-Sharegpt": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/Energetic-Materials-Sharegpt": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/Resistance-Sharegpt": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/Chemical-Biological-Safety-Applications-Sharegpt": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "AquaV/Lit": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/wikitext": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/GiftEvalPretrain": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/lotsa_data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/blip3-kale": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/fineweb_deduplicated": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/xlam-function-calling-60k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/ProVision-10M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/wikisql": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/blip3-ocr-200m": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/blip3-grounding-50m": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/GiftEval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/cota-mantis": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/dialogstudio": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/cos_e": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/cloudops_tsf": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/cota-llava": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/rose": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/FaithEval-unanswerable-v1.0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/ContextualBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Salesforce/program-cota-mantis": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/program-cota-llava": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/FaithEval-counterfactual-v1.0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/summexecedit": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/PROVE": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/FaithEval-inconsistent-v1.0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/CRMArena": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/InstruSum": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/summary-of-a-haystack": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/summedits": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/ttcw_creativity_eval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/sts22-crosslingual-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sts12-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sts14-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sts13-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scifact": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nfcorpus": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/banking77": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arguana": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/emotion": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/sts15-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/imdb": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sickr-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/biosses-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/amazon_massive_scenario": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/amazon_massive_intent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scidocs-reranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/summeval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/twentynewsgroups-clustering": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/stsbenchmark-sts": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/tweet_sentiment_extraction": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/amazon_reviews_multi": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mtop_domain": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mtop_intent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/amazon_counterfactual": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/sts17-crosslingual-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/toxic_conversations_50k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/IndicSentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/msmarco": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hotpotqa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sts16-sts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/told-br": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/biblenlp-corpus-mmteb": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sprintduplicatequestions-pairclassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/results": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/fiqa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/stackoverflowdupquestions-reranking": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "mteb/biorxiv-clustering-s2s": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/dbpedia": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/medrxiv-clustering-s2s": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/fever": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/askubuntudupquestions-reranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/quora": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/climate-fever": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec-covid": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scidocs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mind_small": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/twittersemeval2015-pairclassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/amazon_polarity": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/medrxiv-clustering-p2p": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arxiv-clustering-p2p": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arxiv-clustering-s2s": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/biorxiv-clustering-p2p": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/stackexchange-clustering": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/touche2020": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sib200": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/cqadupstack-tex": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/reddit-clustering": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-android": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-programmers": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-gaming": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-english": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-gis": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/twitterurlcorpus-pairclassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-webmasters": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-unix": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-wordpress": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-physics": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-mathematica": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-stats": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/stackexchange-clustering-p2p": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/reddit-clustering-p2p": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/masakhanews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/stsb_multi_mt": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/tatoeba-bitext-mining": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena-results": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/WikipediaRetrievalMultilingual": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/eurlex-multilingual": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NTREX": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/InstructIR-mteb": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/medical_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/bucc-bitext-mining": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/xnli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/legalbench_corporate_lobbying": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/multilingual-sentiment-classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/AILA_statutes": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/mrtidy": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/indic_sts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/miracl-hard-negatives": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/neuclir-2022": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/multi-hatecheck": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/flores": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/big-patent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mlsum": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/GerDaLIRSmall": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/AILA_casedocs": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/IN22-Gen": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/legalbench_consumer_contracts_qa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/tweet_sentiment_multilingual": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/multilingual-scala-classification": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/germanquad-retrieval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/webis-touche2020-v3": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/IndicQARetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/legal_summarization": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/HotpotQA_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/FEVER_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ClimateFEVER_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/LegalQuAD": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/QuoraRetrieval_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/xnli2.0-multi-pair": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/LeCaRDv2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/arena-wikipedia-7-15-24": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/IndicCrosslingualSTS": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/AmazonReviewsClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/IWSLT2017BitextMining": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/msmarco-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/germanquad-retrieval-qrels": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/XNLIV2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/raw_arxiv": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/IndicReviewsClusteringP2P": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DBPedia_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/swerec_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/IN22-Conv": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/CodeSearchNet-ccr": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/neuclir-2023": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/arena-arxiv-7-2-24": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/wit": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scala_da_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/EcomRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mind_small_reranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DuRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/AskUbuntuDupQuestions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MedicalRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CovidRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NQ_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MSMARCO_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/norec_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/norquad_retrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/VideoRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/raw_medrxiv": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MMarcoRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/raw_biorxiv": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena_emb_wikipedia": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/scala_nn_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CmedqaRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scala_nb_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena-stackexchange": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/mFollowIR": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scala_sv_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/TenKGnadClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/TopiOCQA_validation_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/synthetic-text2sql": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/apps": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/UrduRomanSentimentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/T2Retrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cosqa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/SwedishSentimentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ru_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/codetrans-dl": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_500_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DutchBookReviewSentimentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ar_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_fa_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena-arxiv-7-2-24-samples": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ArxivClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_hi_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_id_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ko_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_en_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_th_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2022Retrieval_fas_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_100_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_5_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/WisesightSentimentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_zh_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_te_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ja_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_es_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_de_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2023Retrieval_fas_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2023Retrieval_zho_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_50_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_2_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_100_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_50_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_5_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mteb-example-submission": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/HotelReviewSentimentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/JSTS": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NQ_PL_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2022Retrieval_zho_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/codefeedback-st": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/codefeedback-mt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_10_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_10_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_2_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_2_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_5_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_2_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/TweetEmotionClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MyanmarNews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/esci": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_fr_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Quora_PL_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2023Retrieval_rus_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2022Retrieval_rus_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ru_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_500_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/trec_covid_top_500_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_100_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_50_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/FilipinoHateSpeechClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_fi_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MSMARCO_PL_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DBPedia_PL_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/HotpotQA_PL_test_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/neuclir-2023-fast": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/stackoverflow-qa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_50_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_10_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_5_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_500_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_100_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nq_top_10_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/PatentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/jaqket": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Quora_PL_validation_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_th_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/TopiOCQA_validation_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ko_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ar_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_fr_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_de_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_en_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DBPedia_PL_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MSMARCO_PL_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_zh_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DBPedia_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/HotpotQA_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/neuclir-2022-fast": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/NeuCLIR2022Retrieval_fas_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2022Retrieval_rus_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2023Retrieval_fas_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2023Retrieval_zho_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_id_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_es_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2022Retrieval_zho_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/codetrans-contest": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Quora_PL_validation_top_250_only_w_correct-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NQ_PL_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Quora_PL_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/HotpotQA_PL_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mteb-lite-run-files-e5": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/RiaNewsRetrieval_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NQ_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MSMARCO_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NeuCLIR2023Retrieval_rus_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_hi_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_fi_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_fa_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_ja_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_th_top_250_only_w_correct_v1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/QuoraRetrieval_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/FEVER_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ClimateFEVER_test_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLRetrieval_te_top_250_only_w_correct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena_emb_arxiv": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/neuclir-2022-hard-negatives": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/neuclir-2023-hard-negatives": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/quora-retrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena_emb_stackexchange": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/mteb-lite-run-files": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-MIRACLRetrieval-th-intfloat_multilingual-e5-large": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-NFCorpus-test-intfloat_e5-small-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cqadupstack-retrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-nq-intfloat-e5-large-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arena-videos": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-MIRACLRetrieval-ja-intfloat_multilingual-e5-large": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-MIRACLRetrieval-de-intfloat_multilingual-e5-large": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-MIRACLRetrieval-fr-intfloat_multilingual-e5-large": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-MIRACLRetrieval-ru-intfloat_multilingual-e5-large": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/index-MIRACLRetrieval-es-intfloat_multilingual-e5-large": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "jat-project/jat-dataset": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "jat-project/jat-dataset-tokenized": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/MultiPL-E": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/MultiPL-E-completions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/CanItEdit": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/npm-follower-data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/engineering-llm-systems": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/MultiPL-T": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/EditPackFT-Multi": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/EditPackFT": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/stack-dedup-python-testgen-starcoder-filter-v2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/stenotype-ts-evl-with-usages": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/manytypes4py": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/stack-dedup-python-testgen-starcoder-filter-inferred-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/humaneval-py-mutants": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/MultiPL-E-synthetic-solutions": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/MultiPL-T-racket-selfinstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/stenotype-eval-js": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nuprl/type-steering": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/stenotype-eval-ts": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nuprl/stack-dedup-python-testgen-starcoder-filter-v2-dedup": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/pass_k_with_MultiPL-E": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/stenotype-results": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/leetcode-js": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/stenotype-ts-training": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/MultiPL-E-pass-rates": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/the-stack-ts": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/stenotype-training": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/stack_dedup_lua_codegen_full": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nuprl/stenotype-ts-eval": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nuprl/JuliaLM": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/passk_with_MultiPL-E": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/MultiPL-E-synthetic-solution": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nyu-mll/glue": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nyu-mll/blimp": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nyu-mll/multi_nli": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nyu-mll/crows_pairs": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nyu-mll/multi_nli_mismatch": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "openai/gsm8k": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openai/openai_humaneval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openai/MMMLU": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openai/summarize_from_feedback": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openai/webgpt_comparisons": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openai/welsh-texts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Zyphra/Zyda-2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Zyphra/Zyda": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Zyphra/dclm-dedup": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "apple/DataCompDR-1B": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "apple/DataCompDR-12M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "apple/DataCompDR-12M-bf16": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "apple/TiC-DataComp": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "apple/GSM-Symbolic": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "apple/mkqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "apple/flair": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "apple/mmau": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "apple/DataComp-12M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google-research-datasets/mbpp": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google-research-datasets/conceptual_captions": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google-research-datasets/nq_open": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google-research-datasets/newsgroup": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/paws-x": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/go_emotions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/natural_questions": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google-research-datasets/paws": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/tydiqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/poem_sentiment": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/wiki_split": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/schema_guided_dstc8": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/discofuse": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google-research-datasets/xquad_r": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/google_wellformed_query": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/wiki_atomic_edits": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/circa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/taskmaster2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/conceptual_12m": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google-research-datasets/cfq": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/aquamuse": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/disfl_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/taskmaster1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/sent_comp": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/crawl_domain": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/qed": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/coached_conv_pref": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/eth_py150_open": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google-research-datasets/taskmaster3": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/kelm": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/coarse_discourse": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/xsum_factuality": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/great_code": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google-research-datasets/totto": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/gap": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google-research-datasets/time_dial": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google-research-datasets/multi_re_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "NTU-NLP-sg/xCodeEval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "stanfordnlp/imdb": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "stanfordnlp/sst2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/snli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/coqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/sst": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/sentiment140": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/SHP": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "stanfordnlp/SHP-2": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "stanfordnlp/squad_adversarial": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/craigslist_bargains": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/concurrentqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/mutual_friends": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/concurrentqa-retrieval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/colorswap": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/xP3x": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "CohereForAI/aya_collection_language_split": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/aya_collection": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "CohereForAI/Global-MMLU": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/Global-MMLU-Lite": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/aya_dataset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/aya_evaluation_suite": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/include-base-44": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/include-lite-44": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/m-ArenaHard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/black-box-api-challenges": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "CohereForAI/lbpp": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_Verified": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_Lite": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/prolong-data-512K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/prolong-data-64K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/llama3-ultrafeedback": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_Lite_oracle": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/QuRatedPajama-260B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "princeton-nlp/datasets-for-simcse": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/gemma2-ultrafeedback-armorm": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_oracle": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/llama3-ultrafeedback-armorm": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/CharXiv": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/LitSearch": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/glue_fairseq_format": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/mistral-instruct-ultrafeedback": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_bm25_40K": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/HELMET": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_bm25_50k_llama": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_Lite_bm25_27K": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_bm25_27K": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_Multimodal": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_Lite_bm25_13K": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_bm25_13K": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/fineweb_edu-swahili-translated": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/LLMBar": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/less_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/TutorChat": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/QuRatedPajama-1B_tokens_for_analysis": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_bm25_27k_cl100k": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/QuRating-GPT3.5-Judgments": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "princeton-nlp/TutorEval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/prolong-ultrachat-64K": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/SWE-bench_gpt4_subset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/Edge-Pruning-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/SWE-bench_oracle_llama": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/SWE-bench_oracle_cl100k": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/QuRating-GPT3.5-Judgments-Test": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "princeton-nlp/TextbookChapters": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/ALCE-data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/SWE-bench_bm25_13k_cl100k": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "princeton-nlp/wikibook_fairseq_format": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/gtr-t5-xxl-wikipedia-psgs_w100-index": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/MoQA": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/ptp_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "LLM360/TxT360": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "LLM360/K2Datasets": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "LLM360/CrystalCoderDatasets": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "LLM360/AmberDatasets": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "LLM360/WebMC": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/HelpSteer2": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/OpenMathInstruct-2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/HelpSteer": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/ChatRAG-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/ChatQA-Training-Data": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/OpenMathInstruct-1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Aegis-AI-Content-Safety-Dataset-1.0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Cosmos-NeMo-Assets": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Daring-Anteater": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/ChatQA2-Long-SFT-data": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/sft_datablend_v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/heb-clip": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/OpenMath-MATH-masked": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/OpenMath-GSM8K-masked": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "togethercomputer/RedPajama-Data-1T-Sample": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "togethercomputer/RedPajama-Data-V2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "togethercomputer/RedPajama-Data-1T": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "togethercomputer/Long-Data-Collections": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "togethercomputer/llama-instruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "togethercomputer/glaive-function-calling-v2-formatted": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "togethercomputer/RedPajama-Data-Instruct": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "togethercomputer/test-glaiveai-function-calling": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/project_gutenberg": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/french-30b": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/old_french_30b_separate": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/colpali-corpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/french-bench-grammar-vocab-reading": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "manu/french_bench_hellaswag": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "manu/french_bench_arc_challenge": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "manu/fquad2_test": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/code_5p_data": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/french_boolq": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/multifquad_test": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/topic_based_nli_test": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/french_5p": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/wikisource_fr": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/french-trivia": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/code-20b": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/code_20b": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/colpali-data-ir": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/old_code_20b_separate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/dila_legifrance": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/english-60b": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/europarl-en-fr": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mldr-zoomed-1000char-2000-queries-documents": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/colpali-queries": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/opus100-en-fr": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/wnut_17": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/dataset_en_fr": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/dataset_1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/french-30b_separate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/code_5p_data_separate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/tabfquad_ir_split_test_200": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/bnf_clean": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/code_20b_separate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/tok-corpus-shuffled": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/dataset_en_fr_short": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mmlu_alpaca": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/few_shot_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/trivia_qa_wiki": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/tok_dataset_1_newtok": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mmlu_alpaca_classic": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/french_poetry": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/french_librispeech_text_only": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/wmt-en-fr": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/tok_dataset_1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/tabfquad_retrieving": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/embedding_data_v2_100k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/embedding_data_v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/embedding_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mmlu_auxiliary_train_formatted_extra": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mmlu_auxiliary_train_unformatted": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/swiss_legislation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/theses_fr_2013_2023": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/french_podcasts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/training_vqa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/pii-masking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/gallica_ocr_cleaned": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/illuin_layout_dataset_text_only": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/REALSumm": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/bnf_gallica": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/croissant_french_dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mldr-zoomed-1000char-2000": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/all_books_test": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/reranker-scores": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/eli5_authorship_attribution": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/canary_split": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/IFTEval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/french_5p_separate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/code_classification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/croissant_dataset_complete": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/m_mmlu": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/m_arc_c": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/m_hellaswag": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/fr_corpora_parliament_processed-lowercased": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "deepseek-ai/DeepSeek-Prover-V1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ByteDance/MTVQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance/FullStackBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance/ComTQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/UltraInteract_sft": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Train-Synthetic-data": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "openbmb/UltraFeedback": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/RLAIF-V-Dataset": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/RLHF-V-Dataset": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Train-In-domain-data": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "openbmb/UltraInteract_pair": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Test-ArxivQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Test-PlotQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Test-ChartQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Test-MP-DocVQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Test-SlideVQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/VisRAG-Ret-Test-InfoVQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/UltraSafety": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/llava_zh": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/VideoChat2-IT": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/AS-100M": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/InternVL-Chat-V1-2-SFT-Data": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/InternVL-Domain-Adaptation-Data": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/GMAI-MMBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenGVLab/Caption-Evaluation-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/Region-Evaluation-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/MMPR": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/InternVideo2_Vid_Text": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/InternVL-SA-1B-Caption": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/InternVid-10M-FLT-INFO": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/InternVid-Full": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/AS-V2": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/AS-Core": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/LORIS": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/MM-NIAH": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/VideoMAEv2-TAL-Features": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/CRPE": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/ScaleVLN": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/MVBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenGVLab/GUI-Odyssey": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/ShareGPT-4o": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/ceval-exam": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/mmlu": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/afqmc_small": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/coco_captions_small_slice": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/cmmlu": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/DuReader_robust-QG": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/coco_2014_caption": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/face_2d_keypoints_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/race": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/Youku-AliceMind": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/ChineseText2SQL": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/chinese-poetry-collection": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/clue": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/ms_ds_meta_jsonlines": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/chatglm_llm_fintech_raw_dataset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/image-portrait-enhancement-dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/wikitext": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/ICASSP_2021_DNS_Challenge": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/Alimeeting4MUG": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_aishell1_testsets": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/Caltech101": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/pets_small": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/muge": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/bbh": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/glue": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/SkyPile-150B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/Libri2Mix_8k": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/Emilia-Dataset": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/person_detection_for_train": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/ai2_arc": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/gsm8k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/afqmc": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/wiki-document-segmentation": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/ocr_fudanvi_zh": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/DreamBooth-Dataset-Finetuning-Examples": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/super_glue": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/aishell1_subset": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/COCO_segmentation_inference": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/hellaswag": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/safetyhelmet_detection_for_train": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/AliMeeting": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/LIP": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/CIHP": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/movie_scene_seg_toydata": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/REDS_video-deblurring": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/PlacesToydataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/ms_hackathon_23_agent_train_dev": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/head_detection_for_train": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/dolma": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/competition_math": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/squad": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/hand_detection_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/truthful_qa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/banking77": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_ja_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_cantonese-CHS_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/cigarette_detection_for_train": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/vqa_trial": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/human_face_portrait_compound_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/head_detection_for_train_tutorial": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/imdb": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/COCO2017_Instance_Segmentation": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/Kolors_awesome_prompts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/dctnet_train_clipart": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/facemask_detection_for_train": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/xnli": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/MVBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_id_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_vi_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_fa_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/mnist": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_en_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/DAMO.NLS.KAN-TTS.OpenDataset": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/COCO2017_panopic_subset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_pt_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/safetyhelmet_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/body_2d_keypoints_test_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/MMBench-Video": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "modelscope/phone_detection_for_train": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_ko_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/trivia_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/cigarette_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/Video-MME": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/image_object_detection_auto_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/P-MMEval": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/cellphone_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/nllb": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/DIV2K": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/MultiWoz2.0": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_ru_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/video_single_object_tracking_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/facemask_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/video_multi_object_tracking_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_es_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/Libri2Mix_8k_test": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/dctnet_train_clipart_mini": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/mug-challenge-test": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/DARTset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/OxfordPets": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/cv_body-3d-keypoints_video_dataset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_de_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/human_portrait_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/speech_kws_mobile_xiaoyun_pos_testsets": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/xtreme": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/coco2017val": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/speech_asr_commonvoice_fr_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/wenetspeech_error_correct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/humaneval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/Video_Description_Editing": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/referring_vos_toydata": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/industry_pdf_report_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/ShanghaiTech-A": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/VIPSeg_subset": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/human_wholebody_keypoint_dataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/ShanghaiTech-B": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/universal_dependencies": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/coco2017": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/plants-image": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/image-colorization-dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/MMLU-Pro": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/HyperlinkMRC": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/MATH-Hard": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/cv_hand-2d-pose-keypoints_coco-wholebody_test": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/single-image-super-resolution-dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/speech_asr_commonvoice_eng_trainsets": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/face-generation-dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/carla-nuscenes": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/traffic_sign_dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/MATH-plus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/gpqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/thuthful_qa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/ms_hackathon_23_multiconcept_dataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/MMLU-STEM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/GenAI-Arena-Image-Edition": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/GenAI-Arena-Image-Generation": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/Entity-Imagen": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/LongICLBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "modelscope/M-BEIR": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/Mantis-Eval": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/person-image-cartoon_train_clipart": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/cv_tracffic_sign_dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/Mantis-Instruct": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Qwen/CC-OCR": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Qwen/ProcessBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/DocHieNet": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Objects365_2020": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/WuDaoCorporaText": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-MTP": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/OL-CC": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/WaterER": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/ArabicText-2022": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/RoadsideDataset": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-IndustryCorpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-IndustryCorpus-v2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/Objects365_2019": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/FakeNews": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/TsinghuaAligner": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/SafetyHelmet": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-TACO": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/ExpertMatching": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/AMinerWhoIsWho": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CrowdHuman2020": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI3": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/NewsSummary": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/InfinityInstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CraneDetection": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/FakeNewsDetect": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI3-HQ": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/ArabicText-NTG": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/MagicSpeechNet": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CAIL": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/CrowdHuman2019": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/URISC": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/TopWORDS": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/TianGongPDR": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/NLPCC2013CLSentimentData": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/OpenAcademicGraph": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/ycylsjj": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Birdsdata": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/SogouSRR": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/INSPEC": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/SogouQCL": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/FigureSkatingDataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/FewRel": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-IndustryInstruction": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/THUUyMorph": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/TianGongST": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/ExpertFinding": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/OAGWhoIsWho02": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/GDB17": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/OAGWhoIsWho01": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_finance_economics": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/Twitter": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_real_estate_construction": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/SportNews": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_law_judiciary": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_biomedicine": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_news_media": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/Particle": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_automobile": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_computer_programming_code": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_literature_emotion": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_water_resources_ocean": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_accommodation_catering_hotel": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_aerospace": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_agriculture_forestry_animal_husbandry_fishery": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_artificial_intelligence_machine_learning": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_computer_communication": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_current_affairs_government_administration": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_electric_power_energy": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_film_entertainment": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_fire_safety_food_safety": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_game": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_mathematics_statistics": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_medicine_health_psychology_traditional_chinese_medicine": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_mining": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_other": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_other_information_services_information_security": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_other_manufacturing": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_petrochemical": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_sports": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_subject_education_education": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_technology_scientific_research": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_tourism_geography": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2_transportation": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/OmniCorpus-CC": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/SA-Med2D-20M": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenGVLab/OmniCorpus-CC-210M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/OmniCorpus-YT": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/InternVid": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/MMT-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "internlm/Lean-Workbook": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "internlm/Agent-FLAN": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "internlm/Lean-Github": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "opendatalab/OmniDocBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "opendatalab/OHR-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "AI4Chem/ChemData700K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "AI4Chem/ChemBench4K": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "AI4Chem/ChemPref-DPO-for-Chemistry-data-en": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "AI4Chem/C-MHChem-Benchmark-Chinese-Middle-high-school-Chemistry-Test": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "AI4Chem/uspto_1k_TPL_processed": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "AI4Chem/uspto_1k_TPL": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "AI4Chem/ChemPref-DPO-for-Chemistry-data-cn": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "AI4Chem/dpo-mix-7k-alpaca-en": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "AI4Chem/MultiCorpus": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lmsys/toxic-chat": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lmsys/lmsys-chat-1m": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "lmsys/chatbot_arena_conversations": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "lmsys/mt_bench_human_judgments": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tiiuae/documentation-images": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "tiiuae/falcon-refinedweb": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "tiiuae/visper": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/xnli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/voxpopuli": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/wiki_dpr": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/belebele": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/flores": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/kilt_tasks": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/mlqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/multilingual_librispeech": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/anli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/md_gender_bias": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/empathetic_dialogues": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/covost2": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/textvqa": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/hand_tracking_challenge_umetrack": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/winoground": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/kilt_wikipedia": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/asset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/PUG_ImageNet": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/emu_edit_test_set": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/gelsight-force-estimation": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/babi_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/imppres": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/digit-pose-estimation": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/curiosity_dialogs": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/Multi-IF": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/digit-force-estimation": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/emu_edit_test_set_generations": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/PUG_Animals": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/neural_code_search": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/lama": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/PUG_SPAR": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/touch-slide": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/content_rephrasing": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/Self-taught-evaluator-DPO-data": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "facebook/mdd": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/fairseq2-lm-gsm8k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/wiki_movies": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/tgve_plus": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/sapiens_toy_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/panda": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/toolverifier": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/pmd": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/feint6k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "THUDM/LongBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "THUDM/LongAlign-10k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/ImageRewardDB": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "THUDM/humaneval-x": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/LongCite-45k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/AlignMMBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "THUDM/BPO": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "THUDM/LVBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "THUDM/LongReward-10k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/LongWriter-6k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/webglm-qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/AgentInstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/CogVLM-SFT-311K": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/self-cognition": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Qwen/P-MMEval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "huawei-noah/python_text2code": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "huawei-noah/human_rank_eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "huawei-noah/entity_cs": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "huawei-noah/CHARP": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/xtreme": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/fleurs": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/IFEval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/boolq": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/xquad": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/wiki40b": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/xtreme_s": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/frames-benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/speech_commands": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/Synthetic-Persona-Chat": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/civil_comments": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_ct_code_to_text": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/jigsaw_toxicity_pred": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/docci": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/MusicCaps": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_tc_text_to_code": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_cc_defect_detection": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_cc_code_to_code_trans": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/IndicGenBench_flores_in": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/dreambooth": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/quickdraw": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/bigbench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/spiqa": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_cc_code_completion_token": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_cc_code_refinement": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/imageinwords": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/wit": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google/code_x_glue_cc_clone_detection_big_clone_bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/fleurs-r": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_tt_text_to_text": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/IndicGenBench_xquad_in": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/code_x_glue_cc_cloze_testing_all": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/trueteacher": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_tc_nl_code_search_adv": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/air_dialogue": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_cc_clone_detection_poj104": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/code_x_glue_cc_code_completion_line": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/coverbench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/granola-entity-questions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/IndicGenBench_xorqa_in": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/code_x_glue_cc_cloze_testing_maxmin": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/jigsaw_unintended_bias": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/reveal": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/cvss": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google/mittens": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/IndicGenBench_crosssum_in": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/red_ace_asr_error_detection_and_correction": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/flame-collection": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google/TACT": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/wikitext_document_level": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/lambada_openai": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "EleutherAI/race": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/hendrycks_math": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "EleutherAI/the_pile_deduplicated": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/drop": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/proof-pile-2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/logiqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/coqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/pile": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/arithmetic": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/rpj-v2-sample": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/asdiv": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/cifarnet": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/hendrycks_ethics": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fever": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/headqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_1000_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/pythia_deduped_pile_idxmaps": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/pythia_pile_idxmaps": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/pythia-memorized-evals": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/pile-deduped-pythia-preshuffled": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_10000_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/lichess-puzzles": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/pile-deduped-pythia-random-sampled": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_15000_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/pile-standard-pythia-preshuffled": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_25000_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/raw_deduplicated_pile": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/twitter-sentiment": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/pile-duped-pythia-random-sampled": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-bios-dataset-25000": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/truthful_qa_binary": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/mutual": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_500_uniform_r17": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/auto_interp_interpretations": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/profiles_dataset_10000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_20000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fake-cifarnet": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_25000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/persona": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_50000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_30000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/qm-grader-last": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/unscramble": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_15000_uniform": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/sycophancy": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/truthful_qa_mc": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/qm-mixture": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_250000": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_25000": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/tinystories-pretokenized-pythia": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-qa-dataset-10000": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-qa-dataset-t0-p0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fake-fashion-mnist": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_2500_uniform": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_10000": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-bios-dataset-250000": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/fake-cifar10": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fake-svhn": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/qm-grader-first": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/profiles_dataset_5000_uniform": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-bios-dataset-25000_shuffled": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-bios-dataset-10000_shuffled": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-bios-dataset-10000": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-qa-dataset-25000": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-qa-dataset-250000": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/naturenet": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fake-mnist": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_multiplication_increment0_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/transformer-reasoning-bios-dataset": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/quirky_population_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fake-naturenet": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/rpj-v2-sample-mixtral": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_extended_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/muInstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_nli_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_authors_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_hemisphere_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_capitals_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0_bob_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0_alice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sentiment_raw": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_population_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_squaring_increment0_alice_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0_bob": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_subtraction_increment0_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_addition_increment0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_bob_hard": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_sciq_alice_easy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/quirky_modularaddition_increment0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/CEBaB": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/advanced_ai_risk": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/bigbench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "EleutherAI/rpj-v2-sample-pretokenized": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/pile-standard-pythia-resharded": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/pile-ngrams": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/winogenerated": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/Infinity-MM": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CCI3-HQ": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/Infinity-Instruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_medicine": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_film": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/TACO": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Objaverse-MIX": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_finance": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/DenseFusion-1M": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CCI2-Data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/COIG": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CCI3-Data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_education": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_politics": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/COIG-PC": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_law": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/COIG-PC-core": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CMMU": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_news": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_sports": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/OPI": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Infinity-Preference": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "BAAI/COIG-PC-Lite": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/DataOptim": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/AquilaMed-Instruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_technology": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Law-Justice": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_literature": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/AquilaEdu-Instruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_automobile": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/Multimodal-Robustness-Benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_travel": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/SVIT": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/JudgeLM-100K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/InfinityMATH": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Aerospace": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_emotion": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_computer": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_programming": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_agriculture": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/CCI-Data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/JudgeLM-data-collection-v1.0": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CCI3-HQ-Annotation-Benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_mathematics": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryCorpus_ai": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/AquilaMed-RL": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Health-Medicine": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Finance-Economics": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Transportation": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/CapsFusion-120M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Hospitality-Catering": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Automobiles": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Travel-Geography": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Literature-Emotions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Subject-Education": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Artificial-Intelligence": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/IndustryInstruction_Technology-Research": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "baidu/TARA": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "baidu/GPTDynamics": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "baidu/rendered_xnli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "baidu/rendered_GLUE": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDriveLab/OpenScene": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDriveLab/LightwheelOcc": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDriveLab/DriveLM": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDriveLab/OpenDV-YouTube-Language": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDILabCommunity/LMDrive": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDILabCommunity/fake_browser_state_zoo": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDILabCommunity/Pong-v4-expert-MCTS": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDILabCommunity/MasterMind": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDILabCommunity/rl_unplugged_dm_control_suite": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.1-70B-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.1-8B-Instruct-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.2-1B-Instruct-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.2-1B-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.2-3B-Instruct-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.1-8B-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.1-405B-Instruct-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.3-70B-Instruct-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.2-3B-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.1-70B-Instruct-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "meta-llama/Llama-3.1-405B-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "alibabasglab/VoxCeleb2-mix": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "alibabasglab/KUL-mix": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "alibabasglab/YGD-mix": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "alibaba-pai/SimpleQA-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HWERI/openorca-multiplechoice-5k-comparisons": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "PaddlePaddle/duconv": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "PaddlePaddle/dureader_robust": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/SafeMTData": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/MiChao": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/mathpile": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/WanJuan1_dot_0": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/LAMM": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/WanJuanCC": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/open-compass-OpenFinData": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/InternVid-10M-FLT": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/DataEngine-InstData": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/VIGC-InstData": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/HalluQA": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/11k_Hands": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/NAIST_COVID": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenDataLab/CrossWOZ": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Clothing1M": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/E-KAR": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/ImageNet-R": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/3D_Hand_Pose": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/ADE20K_2016": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/CityScapes": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/HotpotQA": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Florence3D": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/GAOKAO-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenDataLab/GQA": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/RCTW-17": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/THCHS-30": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Grocery_Store": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ChID": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Geometry3K": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/AIDER": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Fashion-MNIST": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/S3DIS": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Street_Scene": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/AIST_plus_plus": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/IIIT_5K": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Completion3D": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Flickr2K": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/EuroSAT": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MVTecAD": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MOT20": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/RPC": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Paint4Poem": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/DomainNet": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ExDark": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Chart-to-text": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/IconQA": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/MPII_Human_Pose": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ShareGPT91K": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Office-Home": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/STN_PLAD": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Stanford_online_Products": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/DRAM_Diverse_Realism_in_ArtMovements": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ReDial": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/NDISPark_Night_and_Day_Instance_Segmented_etc": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/FSOD": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/MVTEC_3D-AD": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MetFaces": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/SQUID": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Everybody_Dance_Now": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/PathTrack": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/FAT": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/RealNews": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Pix3D": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MOT17": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/SensatUrban": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/HO-3D": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/CUB-200-2011": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/EmpatheticDialogues": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/QST": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Chinese_AI_and_Law_CAIL_2018": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/FVI": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ClipShots": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Disaster": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/HMDB51": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/DTD": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/HRF": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/DMQA": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/PartNet": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/CIFAR-100": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ChartQA": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Kuzushiji-Kanji": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Polyvore": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/GeoQA": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/ScienceQA": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Logic2Text": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/AID": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/DukeMTMC-VideoReID": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/DocBank": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MSRA-TD500": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Visual_Genome_Dataset_V1_dot_2": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/CASIA-HWDB": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/OCHuman": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/KolektorSDD2": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/OSLD": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/SERV-CT": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/GoPro": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "ZhipuAI/AgentInstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/LongBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/ImageRewardDB": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "ZhipuAI/LongAlign-10k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/LVBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/AlignMMBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/webglm-qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/CogVLM-SFT-311K": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/BPO": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "ZhipuAI/humaneval-x": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/LongWriter-6k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/3DHuman_synthetic_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/ms_bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/MSAgent-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/foundation_model_evaluation_benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/people_daily_ner_1998_tiny": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/3DHuman_action_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/GeoGLUE": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/CValues-Comparison": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/controlnet_dataset_condition_fill50k": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/resume_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/ICDAR13_HCTR_Dataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/style_custom_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/weibo_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/nerf_recon_dataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/ms_agent": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/wnut17_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/nlp_polylm_multialpaca_sft": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/GOPRO": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/100PoisonMpts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/msra_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/AnyWord-3M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/chinese-kuakua-collection": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/MSAgent-Pro": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/SAL-HG": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/wnut16_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/imagenet-val5k-image": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/QBQTC": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/ntire23_video_colorization": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT-Chinese-to-English-Machine-Translation-Training-Corpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/toy_msra": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/zh_cls_fudan-news": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/absa_aoe": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT-Chinese-to-English-Machine-Translation-newstest": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/zh_ner-JAVE": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/GridforVGT": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/DocStruct4M": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/KADID-10k-database": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/WMT-Chinese-to-English-Machine-Translation-Medical": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/nlp_domain_classification_chinese_testset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/cv_realtime-image-object-detection_TestDataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/CleanInterlace-video-deinterlace-dataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/MTWI": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/MSAgent-MultiRole": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/AnyText-benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/WebText_Dataset": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/Anti_UAV": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/Layout-Instruction-Data": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/D4LA": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/DocReason25K": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/MIBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/speech_asr_slr80_my_trainsets": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/conllpp_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/BDD100K": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT-English-to-Chinese-Machine-Translation-newstest": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/nlp_style_classification_chinese_testset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/EasyCV_panoptic": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/coco_stuff164k": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/DocDownstream-1.0": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/IWSLT-Chinese-to-English-Machine-Translation-Spoken": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/SIBR": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/IWSLT-English-to-Chinese-Machine-Translation-Spoken": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT-English-to-Chinese-Machine-Translation-Medical": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/VOD_TESTDATASET04": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/game_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/toolbench_for_alpha_umi": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/literature_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/book9_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/address-ner-ccks-2021": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/EgoVid": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/wenetspeech_correct": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/nlp_style_classification_english_testset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/TUL": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/bank_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/multico_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/M-Paper": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/synthetic_multi-view_human_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/cross_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT-Chinese-to-English-Machine-Translation-Training-Corpus-new": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/DocLocal4K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/style_custom_video_dataset_maozhan": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/IWSLT2014-French-to-English-Machine-Translation-Spoken": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/conll03": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/VOD_TESTDATASET": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/wiki_ner": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/body_reshaping_dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/Loong": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/CVPR23_Anti-UAV": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/OD_TRAINTEST": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/DocDownstream-2.0": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT2014-English-to-French-Machine-Translation-newstest": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/MP-DocStruct1M": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/DARTset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/UAV_CVPR23": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/style_custom_dataset_3D_new": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/IWSLT2014-English-to-French-Machine-Translation-Spoken": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WMT2014-French-to-English-Machine-Translation-newstest": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/3DHuman_synthetic_dataset_mini": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/DocGenome12K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/MP-DocReason51K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/WebRPG_Dataset": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/GeoGLUE_GeoETA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/multi-view_human_dataset": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "iic/style_custom_dataset_3D_old": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/style_custom_dataset_3D_comp": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/CVPR23_UAV": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDriveLab/OpenLane": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDriveLab/OpenLane-V2": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDriveLab/CVPR2023-3D-Occupancy": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDriveLab/CVPR24-Occ-Flow-Challenge": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenDILab/MasterMind": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenXDLab/SynBody": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenXDLab/HuMMan": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenXDLab/GTA-Human": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenXDLab/OmniObject3D": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/Kinetics-400": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/Kinetics_700-2020": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/Kinetics600": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/Kinetics_700": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/Kinetics400-skeleton": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/FineGYM-skeleton": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenMMLab/UCF101-skeleton": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenMMLab/AVA-Kinetics": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenMMLab/NTURGB_plus_D-skeleton": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenMMLab/HMDB51-skeleton": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenScienceLab/SEVIR": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenScienceLab/CQ500": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MammalNet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/LAMOST-Spectra-Classification-Dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/LoTE-Animal": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/WU-Minn-HCP-1200-Subjects-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ImageNet-EEG": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Animal-Kingdom": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/BraTS-2021": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/OpenVaccine": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MoleculeNet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Human-Action-Dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/RSNA-Intracranial-Hemorrhage-Detection": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ChemBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenScienceLab/edbo": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/CREMI": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Stanford-Research-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/NCBI%20Reference-Sequence-%20Database": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/FIB-25": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/NONCODE": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/3DTurbulence-DNS": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/BRENDA": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MindBigData2023-MNIST-8B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Single-neuron-datasets-for-mouse-hippocampus": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Yield-Curation-of-USPTO": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/APT-36K": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/DNS-database": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MacaquePose": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Map-of-the-surface-of-the-Moon3": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Pfam": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/CATH-Gene3D-v4-3": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/AIMD-Chig": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/zebra-finch": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Thoughtviz-dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/AirfRANS": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ChIPBase": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Open-catalyst-project": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Four-seasons-star-chart": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/STRING": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Multiband-Astronomical-Observations-and-Data-Processing-Graduate-Course-Optical-Infrared-Data-Processing-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Turbulent-Database-France": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Observation-data-of-the-solar-magnetic-field-in-Huairou-Beijing-from-1987-to-2011": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Stellar-Parameters-for-over-20-Million-Stars-from-SAGES-DR1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/SILVA": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/BigBrain": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/PhosphoSitePlus-v6-7-4": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ZINC": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Cattle": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/BOLD5000": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Pig-PIC": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/US-NAM": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/WTCCC1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/maize-AMES": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Hemibrain": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/UniProt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/bpRNA": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/The-Fifth-US-Naval-Observatory-CCD-Astrograph-Catalog-(UCAC5)": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Himawari-8-L1B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/UNIMOD": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/First-M87-EHT-Results-Calibrated-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/AAVSO-Photometric-All-Sky-Survey-(APASS)-Data-Release-10": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Southern-California-Earthquake-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/HRRR": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ChEBI": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/nRC": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ERA5-np-float": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MultiRM": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/RNAContact-Distance": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/InterPro": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Global-Ocean-Physics-Reanalysis": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/SpliceAI": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Multi-wavelength-Observations-of-M87-During-the-2017-EHT-Campaign": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/LAMOST-Quasar": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/EHT-M87-Polarized-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/The-Third-US-Naval-Observatory-CCD-Astrograph-Catalog-(UCAC3)": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Distances-to-1.47-billion-stars-in-Gaia-EDR3-2021": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/DeepCRISPR-Off": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/EHT-observations-of-Centaurus-A-Calibrated-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Map-of-the-surface-of-the-Moon2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Suzuki-Miyaura-coupling-on-nanomole-scale": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MULTI-RADAR-MULTI-SENSOR-SYSTEM": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/GoFA-(Golang-standards-Of-Fundamental-Astronomy)": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/EHT-Visibility-Amplitude-Data-of-M87*-in-2011-2013": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/SPICE-2-0-1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Protein%20data%20bank": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/CCSLR-Data-Release-2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/First-Sagittarius-A-EHT-Results-Calibrated-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/C-N-cross-coupling": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Time-frequency-Bulletin": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/HSOS-Solar-Multi-Channel-Telescope-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Buchwald-Hartwig-couplings": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/DeepCRISPR-On": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/MPF": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/ConsensusPathDB": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/SAGES-Data-Release-1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Optimus": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Chinese-Ancient-Astronomical-Fundamental-Reference-Star-Catalog": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/FASHI-Data-Release1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/EAGLE": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/RNAContact": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Firs-3C279-EHT-Results-Calibrated-Data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Gold166-v1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/RNA-Switches": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/2017-EHT-Observations-Complete-L1-Data-Products": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/OQMD": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/The-Fourth-US-Naval-Observatory-CCD-Astrograph-Catalog-(UCAC4)": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/CPA-catalyzed-thiol-addition-to-N-acylimines": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Multi-wavelength-Observations-of-Sgr-A-During-the-2017-EHT-Campaign": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/All-historical-observation-data-of-JWST": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/The-First-LHAASO-Catalog-of-Gamma-Ray-Sources": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/APARENT": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/HKO-7": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/The-Second-US-Naval-Observatory-CCD-Astrograph-Catalog-(UCAC2)": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/StrctureImpute": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/1925-2015-sunspot-drawing-in-China": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Deoxyfluorination-with-Sulfonyl-Fluorides": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Chignolin-VisNet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/UCAAC1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/LAMOST8V2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/Conserved-Domain-Database": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenScienceLab/SWISS-2DPAGE": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/InternVL-LaionCOCO-OCR": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/InternVL-WuKong-OCR": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "lmsys/lmsys-arena-human-preference-55k": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "EleutherAI/auto_interp_explanations": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "internlm/Condor-SFT-20K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "internlm/OREAL-RL-Prompts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "opendatalab/ProverQA": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/uco3d": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/CoTracker3_Kubric": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/ComplexFuncBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "THUDM/T1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "THUDM/VisionRewardDB-Video": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "THUDM/VisionRewardDB-Image": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "baichuan-inc/OpenAudioBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "baichuan-inc/OpenMM_Medical": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "EleutherAI/fineweb-edu-dedup-10b": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/erased-svhn": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/reasoning-mix": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/filtering-annealing-mix": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/dclm-dedup-25B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/llama-3.1-tulu-3-405b-preference-mixture": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-sft-olmo-2-mixture-filter-datecutoff": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-mixture-filter-datecutoff": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/llama-3.1-tulu-3-405b-preference-mixture-filter-datecutoff": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/olmoe-0125-1b-7b-preference-mix": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "Salesforce/FinEval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/MSMARCO-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Stats-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MassiveIntentClassification": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeRAGOnlineTutorials": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/BIRCO-DorisMae-Test": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/Touche2020-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Unix-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Webmasters-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Tex-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/BIRCO-ClinicalTrial-Test": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/BIRCO-Arguana-Test": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "mteb/BIRCO-WTB-Test": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/CQADupstack-Mathematica-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Android-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/BIRCO-Relic-Test": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/CQADupstack-Gis-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Physics-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Gaming-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-English-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/SciFact-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DBPedia-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Quora-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/HotpotQA-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/TRECCOVID-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Wordpress-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstack-Programmers-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/SCIDOCS-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ArguAna-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/FiQA-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NQ-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NFCorpus-PL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/climate-fever-v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/verbal-reasoning-challenge": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "stanfordnlp/nnetnav-live": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "stanfordnlp/nnetnav-wa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "princeton-nlp/TextbooksBySubject": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/AceMath-Instruct-Training-Data": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Aegis-AI-Content-Safety-Dataset-2.0": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/AceMath-RM-Training-Data": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/AceMath-RewardBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/CantTalkAboutThis-Topic-Control-Dataset-NC": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/CantTalkAboutThis-Topic-Control-Dataset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/X-Mobility": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/llm-robustness-leaderboard-evals": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/tech_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/eiopa_europa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mldr-zoomed-1000char-total-queries-documents": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/covid_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/narrative_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "manu/mldr-zoomed-1000char-total": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mldr-eval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mldr-zoomed-100char-total-queries-documents": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "manu/mldr-zoomed-100char-total": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "ByteDance/CloudTimeSeriesData": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "ZhipuAI/VisionRewardDB-Video": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "ZhipuAI/VisionRewardDB-Image": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "modelscope/MathR": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/R1-Distill-Math-Test": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "modelscope/MathR-32B-Distill": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/LJSpeech-1.1-48kHz": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDriveLab/AgiBot-World": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/VideoChat-Flash-Training-Data": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "internlm/SWE-Fixer-Train-Editing-CoT-70K": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/WanJuan-Thai": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/WanJuan-Russian": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/WanJuan-Arabic": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/WanJuan-Korean": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/WanJuan-Vietnamese": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/natural_reasoning": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/Wildchat-RIP-Filtered-by-70b-Llama": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "facebook/Wildchat-RIP-Filtered-by-8b-Llama": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "facebook/linguini": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "google/smol": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/wmt24pp": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "google/wmt24pp-images": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/filtering-annealing-mix_20250226-011545": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/dclm-dedup_20250227-004105-filters-only": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/pile-preshuffled-seeds": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/filtering-pretraining-mix": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/polypythias-evals": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/OpenSeek-Synthetic-Reasoning-Data-Examples": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/OpenSeek-Pretrain-Data-Examples": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "alibaba-pai/DistilQwen_100k": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/olmOCR-mix-0225": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/CoSyn-400K": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/CoSyn-point": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/pixmo-docs-0223": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/CogAlign": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/SIMPLE": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/SciDocsRR": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/WikipediaRerankingMultilingual": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MIRACLReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/LoTTE": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ESCIReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/SyntecReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/AlloprofReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MindSmallReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CMedQAv2-reranking": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/MMarcoReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/T2Reranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/RuBQReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/WebLINXCandidatesReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CMedQAv1-reranking": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/BuiltBenchRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/VoyageMMarcoReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/StackOverflowDupQuestions": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "mteb/NamaaMrTydiReranking": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/music-genre": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/BuiltBenchReranking": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/common_voice_20_0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "CohereForAI/AyaVisionBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "CohereForAI/m-WildVision": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "togethercomputer/SFT-stage1-v0.3-mini": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/GRScenes": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/WanJuan-Thai": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/WanJuan-Russian": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/WanJuan-Korean": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/WanJuan-Arabic": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/WanJuan-Vietnamese": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/A3D": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/IconArt": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/MGif": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Metaphorical_Connections": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Live_Comment_Dataset": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Hypersim": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Medical_Segmentation_Decathlon": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/CSFCube": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/ASCEND": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/ChMusic": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Eyecandies": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ArtELingo": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/CoAuthor": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MJSynth_Syn90k": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/ISPRS_Potsdam": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/DDI-100": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenDataLab/CCD": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Country211": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Deep_Fashion3D": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/EVA": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/GOT-10k": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/SVT": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/SegTrack-v2": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/SCIMAT": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/CCPM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/MedDialog": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Objects365": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Image_and_Video_Advertisements": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/CEPSUM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/ForgeryNet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenDataLab/ReCO": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/MSR-VTT": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Chinese_Traditional_Painting_etc": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/PubLayNet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenDataLab/DSSE-200": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/NH-HAZE": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/Douban_Conversation_Corpus": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/MIR-1K": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/OmniBenchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenDataLab/Places365": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/Roof-Image_Dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenDataLab/RealSRSet": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/SIXray": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/MMMU-Reasoning-Distill-Validation": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/DFEC": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/VisualPRM400K": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "OpenGVLab/VisualProcessBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenGVLab/NIAH-Video": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/Mono-InternVL-2B-Synthetic-Data": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/InternVL-eval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/LongVid": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "facebook/BigOBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/cyberseceval3-visual-prompt-injection": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/collaborative_agent_bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/OMAT24": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/dclm-dedup_20250227-004105": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/SmolLM2-135M-10B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/SmolLM-135M-100b": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/SmolLM2-135M-100B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "EleutherAI/dclm-dedup_20250227-004105-raw": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/ShareRobot": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/ChildMandarin": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/SeniorTalk": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "PaddlePaddle/GSM8K_distilled_zh": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-olmo-2-mixture-0225": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/DataDecide-eval-instances": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/olmo-2-0325-32b-preference-mix": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/chatbot-area-preference-dissection": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-math-filtered": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-personas-math-grade-filtered": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/lmarena-100k-long-sample-prompts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/tulu-3-sft-mixture-0225": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/big-reasoning-traces": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceFW/clean-wikipedia": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Salesforce/ContextualJudgeBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpokeN-100-English": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/reasoning-weekly": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "LLM360/MegaMath": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/Llama-Nemotron-Post-Training-Dataset-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-SimReady-Warehouse-01": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-Manipulation-SingleArm": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-Manipulation-Kitchen": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/PhysicalAI-SmartSpaces": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-Manipulation-Objects": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/HelpSteer3": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GraspGen": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim-bkp": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Cosmos-Transfer1-7B-Sample-AV-Data-Example": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nvidia/OpenCodeReasoning": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/compute-eval": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/Scoring-Verifiers": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "manu/bnf_test": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "openbmb/DCAD-2000": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "openbmb/WorkflowLLM": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/NOTSOFAR": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/orca-agentinstruct-1M-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/ms_marco": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/timewarp": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/wiki_qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/orca-math-word-problems-200k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/cats_vs_dogs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/bleeding-edge-gameplay-sample": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/SCBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/MMLU-CF": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/codexglue_method_generation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/PEACE": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/EpiCoder-func-380k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/TemporalBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/Taskbench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/MILP-Evolve": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "microsoft/kitab": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/meta_woz": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/WildFeedback": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "microsoft/ba-calendar": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/VISION_LANGUAGE": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/LCC_java": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/RedStone": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "microsoft/LCC_python": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/LCC_csharp": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "microsoft/mocapact-data": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/MeetingBank-QA-Summary": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/CLUES": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/ChatBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/xglue": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/crd3": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/BiomedParseData": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "microsoft/MeetingBank-LLMCompressed": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/FStarDataSet-V2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/IMAGE_UNDERSTANDING": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/rifts": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/FStarDataSet": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/MAGIC": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/msr_genomics_kbcomp": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/FEA-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/rpr": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "microsoft/msr_sqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/msr_zhen_translation_parity": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/bing_coronavirus_query_set": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/Iter-DPO-Dataset": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "microsoft/ms_terms": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/Eureka-Bench-Logs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/msr_text_compression": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "microsoft/IterPref-dataset": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "microsoft/EpiCoder-meta-features": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "sentence-transformers/s2orc": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-opus-100": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/stsb": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-tatoeba": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-talks": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-ccmatrix": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/amazon-reviews": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/all-nli": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/wikianswers-duplicates": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-opensubtitles": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/paq": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/gooaq": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-bm25": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-jw300": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/npr": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/msmarco-co-condenser-margin-mse-cls-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-msmarco-distilbert-base-v3": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/eli5": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/natural-questions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-distilbert-margin-mse-sym-mnrl-mean-v2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/ccnews": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-msmarco-distilbert-base-tas-b": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-distilbert-margin-mse-cls-dot-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/amazon-qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/embedding-training-data": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/stackexchange-duplicates": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-europarl": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/msmarco-msmarco-MiniLM-L6-v3": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/trivia-qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/wikipedia-en-sentences": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-global-voices": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-news-commentary": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/specter": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/agnews": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/miracl": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/NanoMSMARCO-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoNFCorpus-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoNQ-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-wikimatrix": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/quora-duplicates": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/coco-captions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-distilbert-margin-mse-mean-dot-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/sentence-compression": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/reddit": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/mr-tydi": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/simple-wiki": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-distilbert-margin-mse-cls-dot-v2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/wikihow": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/squad": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/altlex": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/mldr": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/trivia-qa-triplet": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/nli-for-simcse": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/codesearchnet": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/yahoo-answers": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-distilbert-margin-mse-mnrl-mean-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/reddit-title-body": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/msmarco-distilbert-margin-mse-sym-mnrl-mean-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-mpnet-margin-mse-mean-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-wikititles": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/law-gpt": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-hard-negatives": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/dureader": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/hotpotqa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/flickr30k-captions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/msmarco-corpus": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/NanoQuoraRetrieval-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/cmedqa-v2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/quora-duplicates-mining": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/t2ranking": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/NanoSciFact-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/pubmedqa": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/NanoSCIDOCS-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoArguAna-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/parallel-sentences-muse": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/wikipedia-sections": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/NanoTouche2020-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoHotpotQA-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/xsum": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/sql-questions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "sentence-transformers/NanoFiQA2018-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoClimateFEVER-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoFEVER-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/NanoDBPedia-bm25": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/lecard-v2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "sentence-transformers/NQ-retrieval": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/dapr": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "UKPLab/PeerQA": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "UKPLab/sparp": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/SLTrans": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/UKP_ASPECT": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/toxic_conversations": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "UKPLab/m2qa": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "UKPLab/dara": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/hate_speech_offensive": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/liar": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/DARA-Agentbench": {
        "modality": "Language",
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "UKPLab/TexPrax": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/insincere-questions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/amazon_counterfactual_en": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/GraspNet-1Billion": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenDataLab/MuTual": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/WanJuanSiLu2_sft_ko": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenDataLab/WanJuanSiLu2": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "modelscope/LLaVA-R1-100k": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/VACE-Benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/EMER-SFT-0.5B": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Chinese-LiPS": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-InfinityMM": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/BAAI-CCI1": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI4.0-M2-Extra-v1": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI4.0-M2-CoT-v1": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/BAAI-CCI4.0-M2-Base-v1": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OpenGVLab/InternVL-Data": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/MMPR-v1.2": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "OpenGVLab/MMPR-v1.2-prompts": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "OpenGVLab/VisualPRM400K-v1.1": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "OpenGVLab/VisualPRM400K-v1.1-Raw": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "opendatalab/WanJuanSiLu-Multimodal-5Languages": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "opendatalab/WanJuanSiLu-Multimodal-3Languages": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "tiiuae/viscon-1m": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "tiiuae/viscon-100k": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/PE-Video": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/PLM-Video-Human": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/PLM-VideoBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/PLM-Image-Auto": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/PLM-Video-Auto": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "facebook/llamafirewall-alignmentcheck-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "THUDM/SWE-Dev-train": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Qwen/PolyMath": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/SmolLM2-1.7B-stage-4-100B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/SmolLM2-1.7B-stage-4-10B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/SmolLM2-135M-20B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/SmolLM2-1.7B-stage-4-20B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/filtering-pretraining-mix-arrow-format": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/filtering-pretraining-mix-formatted": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/OpenSeek-Pretrain-100B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/CCI4.0-M2-Extra-v1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/OpenSeek-Pretrain-100B-jsonl": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/CCI4.0-M2-Base-v1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/CCI4.0-M2-CoT-v1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/DataDecide-data-recipes": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/DataDecide-eval-results": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/sqa_reranking_eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/olmo-2-0425-1b-preference-mix": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "allenai/tulu-3-do-anything-now-eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/tulu-3-harmbench-eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/tulu-3-trustllm-jailbreaktrigger-eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mlfoundations/cua-idm-frame-data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/APIGen-MT-5k": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/InappropriatenessClassificationv2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuNLUIntentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuToxicOKMLCUPClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SentiRuEval2016": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/assets": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/talemaader_pc": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/english-danish-parallel-corpus": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/scandisent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/bengali_document": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ajgt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/bengali_hate_speech": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/tweet_emotion": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/online_store_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/tweet_sarcasm": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/restaurant_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hotel_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/bengali_sentiment_analysis": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/VieQuADRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoSciFactRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NanoSCIDOCSRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoQuoraRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoNQRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoNFCorpusRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoMSMARCORetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoHotpotQARetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoFiQA2018Retrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoFEVERRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoDBPediaRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoClimateFeverRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoArguAnaRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NaijaSenti": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NYSJudicialEthicsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NQ-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NQ-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NLPTwitterAnalysisClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NLPTwitterAnalysisClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NLPJournalTitleIntroRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NLPJournalTitleAbsRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NLPJournalAbsIntroRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NFCorpus-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NFCorpus-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MultilingualSentiment": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MultiLongDocRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MovieReviewSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Moroco": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MintakaRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MewsC16JaClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MasakhaNEWSClusteringS2S": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MasakhaNEWSClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MarathiNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MalteseNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MalayalamNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MacedonianTweetSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MSMARCO-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/MLQuestions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/MLQARetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MAUDLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LivedoorNewsClustering.v2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/LitSearchRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LinceMTBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LegalReasoningCausalityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsTrafficLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsTortsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsImmigrationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsHousingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsHealthLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsFamilyLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsEstatesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsEmploymentLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsEducationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsDomesticViolenceLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsDivorceLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsCrimeLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsCourtsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsConsumerLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsBusinessLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LearnedHandsBenefitsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LccSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LanguageClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LEMBWikimQARetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LEMBSummScreenFDRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LEMBQMSumRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LEMBPasskeyRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LEMBNeedleRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LEMBNarrativeQARetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LCQMC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KurdishSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KorSarcasmClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KorSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KorHateSpeechMLClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KorHateClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KorFin": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Ko-StrategyQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KlueYnatMrcCategoryClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KlueMrcDomainClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KinopoiskClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KannadaNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KLUE-TC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/JavaneseIMDBClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/JaQuADRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/JaGovFaqsRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/JSICK": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/JDReview": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/JCrewBlockerLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Itacola": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ItaCaseholdClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IsiZuluNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/InternationalCitizenshipQuestionsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/InsurancePolicyInterpretationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IndonesianMongabayConservationClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IndonesianIdClickbaitClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IndicNLPNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IndicLangClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IndicGenBenchFloresBitextMining": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/InappropriatenessClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IFlyTek": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HunSum2AbstractiveRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HotpotQA-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HindiDiscourseClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HinDialectClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HebrewSentimentAnalysis": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HeadlineClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HateSpeechPortugueseClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HamshahriClustring": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HagridRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HALClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GujaratiNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GreenNodeTableMarkdownRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GreekLegalCodeClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GreekCivicsQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GermanSTSBenchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GermanPoliticiansTwitterSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GermanGovServiceRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GermanDPR": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/GeorgianSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GeorgianFAQRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GeoreviewClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/GeoreviewClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FunctionOfDecisionSectionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FrenkSlClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FrenkHrClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FrenkEnClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FrenchBookReviews": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FinancialPhrasebankClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FinToxicityClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FinParaSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FilipinoShopeeReviewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FiQA2018-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FiQA2018-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Farsick": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FaroeseSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FaithDial": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FQuADRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/EstQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/EightTagsClustering.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Diversity6LegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Diversity5LegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Diversity4LegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Diversity3LegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Diversity2LegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Diversity1LegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DefinitionClassificationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Ddisco": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CzechSubjectivityClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CzechSoMeSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CrossLingualSemanticDiscriminationWMT21": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CrossLingualSemanticDiscriminationWMT19": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CorporateLobbyingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLISurvivalOfObligationsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLISharingWithThirdPartiesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLISharingWithEmployeesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIReturnOfConfidentialInformationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIPermissibleCopyLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLINoticeOnCompelledDisclosureLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLINoLicensingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLILimitedUseLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIExplicitIdentificationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ContractNLIConfidentialityOfAgreementLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CanadaTaxCourtOutcomesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADWarrantyDurationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADVolumeRestrictionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADUncappedLiabilityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADThirdPartyBeneficiaryLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADTerminationForConvenienceLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADSourceCodeEscrowLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADRofrRofoRofnLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADRevenueProfitSharingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADRenewalTermLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADPriceRestrictionsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADPostTerminationServicesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADNoticePeriodToTerminateRenewalLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADNonTransferableLicenseLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADNonDisparagementLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADNonCompeteLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADNoSolicitOfEmployeesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADNoSolicitOfCustomersLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADMostFavoredNationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADMinimumCommitmentLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADLiquidatedDamagesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADLicenseGrantLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADJointIPOwnershipLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADIrrevocableOrPerpetualLicenseLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADInsuranceLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADIPOwnershipAssignmentLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADGoverningLawLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADExpirationDateLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADExclusivityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADEffectiveDateLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADCovenantNotToSueLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADCompetitiveRestrictionExceptionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADChangeOfControlLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADCapOnLiabilityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADAuditRightsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADAntiAssignmentLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADAffiliateLicenseLicensorLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUADAffiliateLicenseLicenseeLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CSFDCZMovieReviewSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CLSClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BlurbsClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BlurbsClusteringP2P.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BengaliHateSpeechClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BengaliDocumentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AutoRAGRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Assin2STS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AlloProfClusteringP2P.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FEVER-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/EstonianValenceClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/EightTagsClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DiaBlaBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DeepSentiPers": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DanishPoliticalCommentsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DanFeverRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DalajClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DBpediaClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DBPedia-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DBPedia-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CzechProductReviewSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CyrillicTurkicLangClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Core17InstructionRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CodeTransOceanDL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeTransOceanContest": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeSearchNetRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeSearchNetCCRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeRAGProgrammingSolutions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeRAGLibraryDocumentationSolutions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CodeFeedbackST": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CodeFeedbackMT": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CodeEditSearchRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Cmnli": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ClimateFEVER-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ClimateFEVER-Fa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ChemNQRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ChemHotpotQARetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CataloniaTweetClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CUREv1": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CSFDSKMovieReviewSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackWordpressRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackWordpress-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackWebmastersRetrieval-Fa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackWebmasters-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackUnixRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackUnix-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackTexRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackTex-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackStatsRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackStats-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackProgrammersRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackProgrammers-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackPhysicsRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackPhysics-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackMathematicaRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackMathematica-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackGisRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackGis-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackGamingRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CQADupstackGaming-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackEnglishRetrieval-Fa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackEnglish-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackAndroidRetrieval-Fa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CQADupstackAndroid-NL": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/COIRCodeSearchNetRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CLSClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CLSClusteringP2P.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CLSClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CEDRClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CDSC-R": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CDSC-E": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CBD": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BulgarianStoreReviewSentimentClassfication": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BuiltBenchClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BuiltBenchClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BrightLongRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BornholmBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BlurbsClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BlurbsClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BeytooteClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BengaliSentimentAnalysis": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BSARDRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BQ": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ArmenianParaphrasePC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ArguAna-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ArguAna-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AppsRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/AngryTweetsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AlloprofRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AlloProfClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AlloProfClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AlloProfClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AllegroReviews": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AfriSentiClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ATEC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AFQMC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openai/mrcr": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openai/graphwalks": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/OpenMathReasoning": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/ClimbLab": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/Llama-Nemotron-Post-Training-Dataset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/describe-anything-dataset": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/ClimbMix": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/Nemotron-CrossThink": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/dynpose-100k": {
        "modality": "Vision",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Nemotron-MIND": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/OpenCodeInstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/When2Call": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/DLC-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "togethercomputer/together-search-bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "deepseek-ai/DeepSeek-ProverBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "bytedance-research/ToolHop": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "bytedance-research/DeepHall-data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "ByteDance-Seed/Multi-SWE-bench_trajs": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/Multi-SWE-bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/Multi-SWE-RL": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ByteDance-Seed/MAGACorpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "ByteDance-Seed/Multi-SWE-bench_mini": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/VPData": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "TencentARC/SEED-Bench-R1": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/VPBench": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "TencentARC/StoryStream": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "TencentARC/Plot2Code": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/MiraData": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "microsoft/echelon-original-eef-main_right_wrist": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/CoSApien": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/CoSAlign-Train": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "microsoft/echelon-original-ja-main_and_right_wrist": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/CoSAlign-Test": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "open-r1/DAPO-Math-17k-Processed": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "open-r1/codeforces-submissions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "open-r1/verifiable-coding-problems-python_decontaminated-tested": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenDataLab/WanJuanSiLu2O": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "modelscope/EvalScope-Qwen3-Test": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/SEED-Bench-2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/SEED-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/SEED-Bench-H": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/SEED-Bench-2-plus": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/MATH-500": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/aime_2024": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/ultrachat_200k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/ultrafeedback_binarized": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/blogpost-images": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/no_robots": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/stack-exchange-preferences": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/llava-instruct-mix-vsft": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/helpful-instructions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/orca_dpo_pairs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/databricks_dolly_15k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/CodeAlpaca_20K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/Llama-3.2-1B-Instruct-beam-search-completions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/Bespoke-Stratos-17k": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/hhh_alignment": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/testing_alpaca_small": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/instruct_me": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/testing_self_instruct_small": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/mt_bench_prompts": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/testing_codealpaca_small": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/rlaif-v_formatted": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/deita-10k-v0-sft": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/instruction-dataset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/cherry_picked_prompts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/Llama-3.2-3B-Instruct-beam-search-completions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/capybara": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/cai-conversation-harmless": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/grok-conversation-harmless": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/MATH": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/numina-deepseek-r1-qwen-7b": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/Llama-3.2-1B-Instruct-best-of-N-completions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/OpenHermes-2.5-1k-longest": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/prm800k-trl-dedup": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/helpful-anthropic-raw": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/hh-rlhf": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/Koala-test-set": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/Llama-3.2-1B-Instruct-DVTS-completions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/helpful_instructions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/lima_llama2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/summarize-from-feedback": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/Code-Feedback": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/cai-conversation-harmless-old": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/rs_test": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/pmp-se-test-dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/grok-conversation-harmless2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/instruction-pilot-outputs-greedy": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/scale_prompts_098236": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/testing_h4": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/grok-conversation-harmless-old": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/SystemChat": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/code_evaluation_prompts": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/OpenHermes-2.5-preferences-v0-deduped": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/ifeval-like-data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/Llama-3.2-3B-Instruct-best-of-N-completions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/self-instruct-seed": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/self_instruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/spin-ultrachat-prompts-qwen-1.5-0.5b-iter0-iter1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/helpful_instructions_splits": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/cai-conversation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/airoboros-3.2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/summarize_from_feedback": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/test-dataset-all-splits": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/10k_prompts_ranked": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/orca-math-word-problems-200k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/hh-rlhf-h4": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/helpful-self-instruct-raw": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/numina_60k_math_verify_correct_2_4gens_with_rm_scores": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/Llama-3.2-3B-Instruct-DVTS-completions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/instruction-pilot-outputs-sampling": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceH4/h4-anthropic-hh-rlhf-helpful-base-gen": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/h4-tests-format-sft-dataset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/self-instruct-eval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/deita-6k-v0-sft": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/Magpie-Pro-DPO-100K-v0.1-Prompts": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/instruction-pilot-outputs-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/test-cot": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/scale-pm-pilot": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/orca_dpo_pairs_no_system_prompt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/pythia-70m-rs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/aws-pm-pilot": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "HuggingFaceH4/h4-tests-format-dpo-dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/h4_10k_prompts_ranked_gen": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/handbook-images": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceH4/surge-pm-pilot": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "open-r1/OpenR1-Math-220k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/codeforces-cots": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/codeforces": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/verifiable-coding-problems-python": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/ioi-2024-model-solutions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "open-r1/Big-Math-RL-Verified-Processed": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "open-r1/ioi": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "open-r1/OpenThoughts-114k-math": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/OpenR1-Math-Raw": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "open-r1/ioi-sample-solutions": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "open-r1/ioi-test-cases": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "open-r1/verifiable-coding-problems-python_decontaminated": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "open-r1/ioi-cots": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "open-r1/verifiable-coding-problems-python_decontaminated-tested-shuffled": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "open-r1/s1K-1.1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/OpenThoughts-114k-Code_decontaminated": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/SYNTHETIC-1-SFT-Data-Code_decontaminated": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/cosmopedia": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/images": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/finemath": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/dclm-edu": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/smollm-corpus": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/smoltalk": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/cosmopedia-100k": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/smol-smoltalk": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/stack-edu": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/issues-kaggle-notebooks": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/everyday-conversations-llama3.1-2k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/math_tasks": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceTB/bisac_expanded_final": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/MATH": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/finemath_contamination_report": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/ultrachat_questions_about_world": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/OpenHermes-2.5-H4": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/MagPie-Pro-300k-MT": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/miscellaneous": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/sample_log_probs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/instruct-data-basics-smollm-H4": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/SmolLM2-intermediate-evals": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceTB/python-edu-annotations": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/wiki_applied_sciences_college_students_1k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/self-oss-instruct-sc2-H4": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/cosmopedia-20k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/cosmopedia-meta": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/openhermes_filtered": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/Magpie-Pro-300K-Filtered-H4": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/web_under_line_mean_100": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/openstax_paragraphs": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/cosmopedia_6M": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/wiki_natural_sciences_college_high_school_students_1k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/cosmopedia_stanford_openstax_wiki_1k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/cosmopedia_web_textbooks": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/bisac_expanded_topics": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/bisac-topics": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/cosmopedia_web_textbooks_all_2B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/eval_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "open-r1/verifiable-coding-problems-python-10k": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "open-r1/verifiable-coding-problems-python-10k_decontaminated": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "opendatalab/K12textbook": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "tiiuae/Falcon-Arabic-7B-Base-details": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "tiiuae/Falcon-Arabic-7B-Instruct-details": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "facebook/TAPAS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/multiloko": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "google/svq": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/cua-v0-metadata": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/nnetnav-live-uitars": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Salesforce/CRMArenaPro": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nuprl/BigCodeBench-MultiPL-Results": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "stanfordnlp/web_questions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "stanfordnlp/wikitablequestions": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/MIRACLRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/tweet_topic_single": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NusaTranslationBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BrightRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OpusparcusPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/yelp_review_full": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Caltech101": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/yahoo_answers_topics": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/wikipedia_crystallography_analytical": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/wikipedia_theoretical_applied": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/toxic_chat": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sds_gloves": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/PublicHealthQA": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/wikipedia_bio_met_chem": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/DigikalamagClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NusaParagraphTopicClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NusaParagraphEmotionClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NusaX-senti": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PolEmo2.0-OUT": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PpcPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NusaXBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/czech_so_me_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/PersonalJurisdictionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PubChemSMILESBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PolEmo2.0-IN": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/sds_eye_protection": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/csfdcz_movie_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PoemSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PlscClusteringP2P.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PersianFoodSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PAC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OnlineShopping": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DigikalamagClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FalseFriendsDeEnPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PhincBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PlscClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OverrulingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/wikipedia_chem_fields": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/czech_product_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NevIR": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/angry_tweets": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/NordicLangClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/danish_political_comments": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/KLUE-STS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PersianWebDocumentRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PersianTextTone": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PersianTextEmotion": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NollySentiBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NarrativeQARetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/d_bpedia": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ddisco_cohesion": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/PROALegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115InternationalAndSpecificAudiencesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/frenk_en": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/dk_hate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/OdiaNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115PolicyChangeLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115DataRetentionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NorwegianParliamentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/News21InstructionRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/financial_phrasebank": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/PUGGRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PSC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Ocnli": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115UserChoiceControlLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115UserAccessEditAndDeletionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PAWSX": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OralArgumentQuestionPurposeLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115DoNotTrackLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115ThirdPartySharingCollectionLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115DataSecurityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OPP115FirstPartyCollectionUseLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NorwegianCourtsBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NepaliNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/OnlineStoreReviewSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/NanoTouche2020Retrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "LLM360/guru_RL": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "LLM360/guru_preview": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "LLM360/guru": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "LLM360/guru_RL_verl": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nvidia/OpenCodeReasoning-2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Cosmos-Reason1-SFT-Dataset": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Cosmos-Reason1-RL-Dataset": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/miracl-vision": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/AceReason-Math": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Spatial-Intelligence-Warehouse": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Cosmos-Reason1-Benchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/Nemotron-PrismMath": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/LiveCodeBench-CPP": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/GEN3C-Testing-Example": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/TechQA-RAG-Eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/hifitts-2": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "togethercomputer/MoAA-SFT": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "UKPLab/HaluQuestQA": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "open-r1/Mixture-of-Thoughts": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "open-r1/details-open-r1_OpenR1-Distill-7B": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/simplewiki-pruned-350k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/stack-edu-prompts-16langs-1k": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/bisac_topics_expanded_2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/CS-Dialogue": {
        "modality": "Speech",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "BAAI/RefSpatial-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "BAAI/LiveOCRVQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "BAAI/TRUE": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "BAAI/Video-SafetyBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OpenDriveLab/MTGS": {
        "modality": "Vision",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "alibaba-pai/OmniThought": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "alibaba-pai/DistilQwen_1M": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/reward-bench-2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/olmOCR-bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/olmOCR-pes2o-0225": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/sciriff-yesno": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/IF_multi_constraints_upto5": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/qasper-yesno": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/SimpleToM-rich": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/PRISM": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/reward-bench-2-results": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/ruler_data": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/WildChat-4M": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/WildChat-4M-Full": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/filtering-annealing-mix_20250507-012346": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/filtering-pretraining-mix_20250516-0250": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "ByteDance/WildDoc": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance/BandwidthEstimationDataset": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "bytedance-research/Web-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/BM-6M": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ByteDance-Seed/mga-fineweb-edu": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "ByteDance-Seed/Code-Contests-Plus": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/BM-6M-Demo": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ByteDance-Seed/BM-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/Video-Holmes": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/Ultra-FineWeb": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "openbmb/CAGUI": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/mediflow": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/llmail-inject-challenge": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/lost_in_conversation": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/templatic_generation_tasks": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/tsp": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/sat": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/hntt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Shanghai_AI_Laboratory/Lean-Github": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/Lean-Workbook": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/Condor-SFT-20K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/SWE-Fixer-Eval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Shanghai_AI_Laboratory/Agent-FLAN": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/SWE-Fixer-Train-Editing-CoT-70K": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Shanghai_AI_Laboratory/SWE-Fixer-Train-110K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shanghai_AI_Laboratory/OREAL-RL-Prompts": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "iic/longbench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "iic/DocQA-RL-1.6K": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/ruler-128k-subset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/ZeroSearch_dataset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/frames": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "iic/ViSpeak-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/ViSpeak-Instruct": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "iic/docmath": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "iic/ActionArt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/VRBench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "OpenGVLab/MMBench-GUI": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "opendatalab/SlimPajama-Meta-rater": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/WanJuan-BaiHua": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/Meta-rater-PRRC-Rater-dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "opendatalab/SlimPajama-Meta-rater-Reasoning-30B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/SlimPajama-Meta-rater-Readability-30B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/SlimPajama-Meta-rater-Cleanliness-30B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/SlimPajama-Meta-rater-Professionalism-30B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "opendatalab/awesome-markdown-ebooks": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "facebook/IntPhys2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/minimal_video_pairs": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/bouquet": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/seamless-interaction": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "THUDM/VisionRewardDB-Image-regression": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "google/DOCCI-Critique": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google/gemma3n-slicing-configs": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "EleutherAI/first_half_math": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/mmlu_auxiliary_train_formatted_cloze_20250619-1417": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/mmlu_auxiliary_train_formatted_cloze_20250619-1339": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/mmlu_auxiliary_train_formatted_cloze_20250619-1406": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/djinn-problems": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/wmdp_bio_cloze": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/djinn-problems-v0.2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/wmdp_biio_robust_mcqa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/wmdp_bio_robust_mcqa": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/mmlu_test_task_training_mix": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/deep_ignorance_filtered_documents_backup": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/deep_ignorance_blocklist": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/deep_ignorance_filtered_documents": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/djinn-problems-v0.3": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/MCQ_Wiki_-decontaminated_shard_0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/MCQ_Wiki_-decontaminated_shard_2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/MCQ_Wiki_decontamination_report_shard_0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/MCQ_Wiki_decontamination_report_shard_2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/Falcon-details": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/SurveyScope": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ernie-research/TARA": {
        "modality": "Embodied",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "ernie-research/GPTDynamics": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ernie-research/rendered_xnli": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ernie-research/rendered_GLUE": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "alibaba-pai/OmniThought-0528": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "allenai/DataDecide-ppl-results": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/IF_multi_constraints_upto5_no_lang": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/IF_sft_data_verified": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/omega-transformative": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/omega-compositional": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/omega-explorative": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/aime-2021-2025": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/IFBench_test": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/IFBench_multi-turn": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/IFBench_multi-turn_responses_example": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/verifiable-reasoning-filtered-gpt-41": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/math-meta-reasoning-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/code-meta-reasoning-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/verifiable-reasoning-filtered-o4-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/code-meta-reasoning-cleaned": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/math-meta-reasoning-cleaned": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/verifiable-reasoning-filtered-o4-mini-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/verifiable-reasoning-filtered-gpt-41-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/persona-precise-if-r1-merged-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/persona-precise-if-r1-merged-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/coconot-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/coconot-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tulu_v3.9_wildchat_100k_english-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tulu_v3.9_wildchat_100k_english-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/wildchat-r1-p2-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/wildchat-r1-p2-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/wildjailbreak-r1-v2-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/wildjailbreak-r1-v2-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/wildguardmix-r1-v2-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/wildguardmix-r1-v2-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/oasst1-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/oasst1-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/aya-100k-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/aya-100k-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/open_math_2_50k_r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/open_math_2_50k_r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/sciriff_10k_r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/sciriff_10k_r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tablegpt_r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/tablegpt_r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/numinatmath-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/numinatmath-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/the-algorithm-python-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/the-algorithm-python-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/acecoder-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/acecoder-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/rlvr-code-data-python-r1-format-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/rlvr-code-data-python-r1-format-filtered-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/mid-training-OpenMathReasoning-rewrite-teacher-student-lecture-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mlfoundations/cua-v0-1-metadata": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/BibleNLPBitextMining": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/StackOverflowQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SNLHierarchicalClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/BelebeleRetrieval": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ARCChallenge": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/AlphaNLI": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ArEntail": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Assin2RTE": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PawsXPairClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SemRel24STS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/WebFAQBitextMiningQAs": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/PubChemWikiPairClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Robust04InstructionRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RiaNewsRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TV2Nordretrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotTopicsRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/IndicXnliPairClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SIDClustring": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/StatcanDialogueDatasetRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SweFaqRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SwednRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TRECCOVID-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL3Fact": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDMedXpertQAExamRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDDTrainingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCIDOCS-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SDSEyeProtectionClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SICKFr": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SKQuadRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/STSB": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SciFact-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SwissJudgementClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SyntheticText2SQL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL2Pure": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/LegalBenchPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Quail": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/QuoraRetrieval-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RTE3": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RomaniBibleClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RomanianReviewsSentiment": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuSciBenchGRNTIClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDBPAccountabilityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDDAccountabilityLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SDSGlovesClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SICK-E-PL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SlovakMovieReviewSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpanishNewsClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpanishPassageRetrievalS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAJealousy": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotRAGFAQPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotToneUserClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TelemarketingSalesRuleLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL2Context": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL3Context": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CExaPPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/KLUE-NLI": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PunjabiNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDBioinformaticsRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDIIYiClinicalRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDMedicalSciencesRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDPMCClinicalRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RARbCode": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuSTSBenchmarkSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuSciBenchOECDClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuSciBenchOECDClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCIDOCS-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SICK-BR-STS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SIQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SadeemQuestionRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SciFact-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SentimentAnalysisHindi": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SlovakSumRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SwednClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAAnger": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSALove": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotRAGSumSRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotRAGTopicsRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerQAPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SNLHierarchicalClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SNLClustering": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TwitterHjerneRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SNLRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SyntecRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TRECCOVID-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TamilNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TbilisiCityHallBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL3Pure": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TextualismToolPlainLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ThuNewsClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/CTKFactsNLI": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/DKHateClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FarsTail": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ParsinluEntail": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PubChemAISentenceParaphrasePC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PubChemWikiParagraphsPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDBiologyRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDPMCTreatmentRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RUParaPhraserSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RomaTalesBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RomanianSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuBQRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuReviewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SAMSumFa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDBPAuditsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDBPCertificationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDDCertificationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDDVerificationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SRNCorpusBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/STSES": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SanskritShlokasClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SensitiveTopicsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SouthAfricanLangClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpanishSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpartQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAFriendship": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSASadness": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotRAGFAQRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotRAGToneChatbotClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotRAGToneUserClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerQARetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerTextKeywordsPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerTextToneClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TNews": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL1": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TempReasonL2Fact": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TenKGnadClusteringS2S.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ThuNewsClusteringP2P.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FalseFriendsGermanEnglish": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/HellaSwag": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PIQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PubChemSMILESPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Quora-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/R2MEDMedQADiagRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RARbMath": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RonSTS": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/RuSciBenchGRNTIClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDBPTrainingLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDBPVerificationLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SCDDAuditsLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SICK-R-PL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SIDClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SentimentDKSF": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SinhalaNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpanishPassageRetrievalS2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SwahiliNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SwednClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAFear": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSASatisfaction": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSASurprise": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAToneChatbotClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAToneUserClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotSatisfactionLevelClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotToneChatbotClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TERRa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/FarsiParaphraseDetection": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ParsinluQueryParaphPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/PubChemSynonymPC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Query2Query": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SICK-BR-PC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SinhalaNewsSourceClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SiswatiNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SlovakHateSpeechClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SpanishNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TeluguAndhraJyotiNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TenKGnadClusteringP2P.v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TextualismToolDictionariesLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/QBQTC": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotConvSAHappiness": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/SynPerChatbotSumSRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TurHistQuadRetrieval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TweetTopicSingleClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/ToxicChatClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/WRIMEClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Touche2020-Fa": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TswanaNewsClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/UnfairTOSLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/VieMedEVBitextMining": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/VieStudentFeedbackClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Touche2020-NL": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TurkicClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TurkishMovieSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/UkrFormalityClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/VGHierarchicalClusteringP2P": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/VGHierarchicalClusteringS2S": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TopiOCQA": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/UCCVCommonLawLegalBenchClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/Waimai": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/TurkishProductSentimentClassification": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/common_voice_21_0": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/voxpopuli-accent-clustering": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/WebFAQBitextMiningQuestions": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/czech_subjectivity": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/german_politicians_twitter_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ten_k_gnad": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/greek_legal_code": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/estonian_valence": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/filipino_hate_speech": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/syn_per_text_tone": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/filipino_shopee_reviews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sid": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/deep_senti_pers": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/persian_text_emotion": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sentiment_dksf": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nlp_twitter_analysis": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/persian_food_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/fin_toxicity": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/french_book_reviews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/movie_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/gujarati_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hebrew_sentiment_analysis": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hindi_discourse": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sentiment_analysis_hindi": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/frenk_hr": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/indonesian_id_clickbait": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/indonesian_mongabay_conservation": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/italian_linguistic_acceptability": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/javanese_imdb": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/wrime": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/kannada_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/klue_tc": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/klue_tcv2": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/kor_hate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/kor_sarcasm": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/kurdish_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/malayalam_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/marathi_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/macedonian_tweet_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/myanmar_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nepali_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/dutch_book_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/no_rec": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/norwegian_parliament": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/odia_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/pol_emo2_in": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/pol_emo2_out": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/romanian_reviews_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/romanian_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/georeview": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/headline": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/inappropriateness": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ru_reviews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ru_toxic_okmlcup": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/senti_ru_eval2016": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sanskrit_shlokas": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sinhala_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sinhala_news_source": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/csfdsk_movie_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/tut-acoustic-scenes-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/slovak_hate_speech": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/frenk_sl": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/spanish_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/spanish_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/siswati_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/slovak_movie_review_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/swahili_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/dalaj": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/swe_rec": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/swedish_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/tamil_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/telugu_andhra_jyoti_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/wisesight_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/tswana_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/turkish_movie_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/turkish_product_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ukr_formality": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/urdu_roman_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/vie_student_feedback": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/t_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/i_fly_tek": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/multilingual_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/jd_review": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/online_shopping": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/yue_openrice_review": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/isi_zulu_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/commonlanguage-gender-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/commonlanguage-age-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/commonlanguage-lang-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nsynth-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ambient-acoustic-context-small": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/beijing-opera": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/birdclef25-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/crema-d": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/esc50": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/free-spoken-digit-dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/gtzan-genre": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/GunshotTriangulationHear": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/iemocap": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/libricount": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mridingham-stroke": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/mridingham-tonic": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/speech-commands-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/SpokenQA_SLUE": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/vocalsound": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/voxceleb-sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/voxlingua107-top10": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/voxpopuli-accent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/voxpopuli-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sib-fleurs-multilingual-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/minds14-multilingual": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/allegro_reviews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/arxiv": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/j_crew_blocker_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/legal_reasoning_causality_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/maud_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/opp115_data_security_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/opp115_do_not_track_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/opp115_user_choice_control_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/oral_argument_question_purpose_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/overruling_legal_bench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/patent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/poem_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/toxic_conversations": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/wikipedia_comp_chem_spectroscopy": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/voxpopuli-accent-mini": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "LLM360/guru-RL-92k": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "LLM360/guru-RL-92k-extra-info-compressed": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "nvidia/Nemotron-Personas": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Drive-Dreams": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/AceReason-1.1-SFT": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GR00T-Teleop-Sim": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GR00T-Teleop-G1": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/OpenScience": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GR00T-GR1": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/OpenCodeGeneticInstruct": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-GR00T-Eval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Autonomous-Vehicle-Cosmos-Synthetic": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "nvidia/cvdp-benchmark-dataset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/PBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/PhysicalAI-DigitalCousin-Assets": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Autonomous-Vehicles-NuRec": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "nvidia/PhysicalAI-Robotics-NuRec": {
        "modality": "Embodied",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "manu/seen_canaries": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/ScienceOlympiad": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/BeyondAIME": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tencent/WildSpeech-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tencent/C3-BenchMark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tencent/ArtifactsBenchmark": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "openbmb/RLPR-Train-Dataset": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "openbmb/RLPR-Evaluation": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/Updesh_beta": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/Do-You-See-Me": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "sentence-transformers/msmarco-scores-ms-marco-MiniLM-L6-v2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/sounddescs_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/sounddescs_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/afri_senti": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/indic_sentiment": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/masakha_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nusa_paragraph_topic": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/amazon_reviews": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/massive_scenario": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/audiocaps_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/LibriTTS_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/massive_intent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nusa_x_senti": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/indic_nlp_news": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nusa_paragraph_emotion": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/multi_hate": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/audiocaps_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MusicCaps_a2t": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/naija_senti": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Urbansound8K_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ru_nlu_intent": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/cyrillic_turkic_lang": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/Urbansound8K_t2a": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mteb/audioset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/gigaspeech_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MusicCaps_t2a": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/EmoV_DB_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ESC50_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/language": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/catalonia_tweet": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hifi-tts_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hifi-tts_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/gigaspeech_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/nordic_lang": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CMU_Arctic_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/LibriTTS_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MACS_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/MACS_t2a": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/EmoV_DB_t2a": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "mteb/afri_senti_lang": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/hin_dialect": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/turkic": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/JL-Corpus_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/indic_lang": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/ESC50_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/JL-Corpus_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/audioset_strong_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/audioset_strong_a2t": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/scala": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "mteb/CMU_Arctic_t2a": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/olmOCR-mix-0225-benchmarkset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/omega-problems": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/dolma-reddit-to-flashcards-0625": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "allenai/open_math_2_50k_r1-original": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/open_math_2_50k_r1-original-filtered": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "allenai/omega-500": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "allenai/code-meta-reasoning-cleaned-final-string-id": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/smoltalk2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/smollm3-blueprint": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "HuggingFaceTB/stackexchange_2025_md": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/smoltalk-multilingual8-Qwen3-32B-main-gen": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "HuggingFaceTB/smollm3-configs": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "HuggingFaceTB/reasoning_tulu_chatml_580k_all_lengths": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "openbmb/DensingLaw-ScalingBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/Multi-SWE-bench-flash": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/WideSearch": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ByteDance-Seed/cudaLLM-data": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Skywork/SkyPile-150B": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "Skywork/Skywork-OR1-RL-Data": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "Skywork/Skywork-Reward-Preference-80K-v0.2": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "Skywork/CSVQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Skywork/Skywork-Reward-Preference-80K-v0.1": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "Skywork/mock_gsm8k_test": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Skywork/A2-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Skywork/LiveCodeBench": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "Skywork/ChineseDomainModelingEval": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "TencentARC/ShortVid-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "OmniGen2/X2I2": {
        "modality": "Multimodal",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "OmniGen2/OmniContext": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "EleutherAI/SmolLM2-135M-10B-tmp-idx": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/deep-ignorance-pretraining-mix": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/deep-ignorance-annealing-mix": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "EleutherAI/early_unlearning_mixed_tampering_dataset": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/rStar-Coder": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/NextCoderDataset": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/NextCoderDataset-Conversational": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "microsoft/Dayhoff": {
        "modality": "Language",
        "lifecircle": "Pre-training",
        "is_valid": true
    },
    "microsoft/ntt-icml2021": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "microsoft/SynTrail": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "microsoft/prototypical-hai-collaborations": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/Doc-750K": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "OpenGVLab/OpenCUA_Env": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/Ag-LiveCodeBench-X": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nuprl/agnostics-misc": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tiiuae/SyntheticQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tiiuae/NativeQA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tiiuae/NativeQA-RDP": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "tiiuae/evalplus-arabic": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mistralai/MM-MT-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mistralai/mmlu_speech": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mistralai/triviaqa_speech": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "mistralai/gsm8k_speech": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Shitao/MLDR": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Shitao/bge-m3-data": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shitao/bge-reranker-data": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Shitao/tmp_ckpt": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/CI-VID": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/Emotiontalk": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "BAAI/VideoXL2_Training_Data_Anno_Files": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "BAAI/ShareRobot-Bench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/community-alignment-dataset": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "facebook/FACTORY": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "facebook/AbstentionBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "moonshotai/Kimi-Audio-GenTest": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/Multilingual-Thinking": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "HuggingFaceH4/tsm-multilingual": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "nvidia/Nemotron-Post-Training-Dataset-v1": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/AudioSkills": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/Nemotron-Math-HumanReasoning": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/AF-Chat": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/AF-Think": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/LongAudio": {
        "modality": "Speech",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/OpenScienceReasoning-2": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "nvidia/nvblox": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "google/scin": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "Salesforce/ReasoningJudgeBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Salesforce/HERB": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Salesforce/shared-imagination": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "Salesforce/WAFER-QA": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/LongBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/LongBench-v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/CC-Bench-trajectories": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/MotionBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/humaneval-x": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/LongAlign-10k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/ComplexFuncBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/AlignMMBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/LVBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/ImageRewardDB": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "zai-org/LongWriter-6k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/AgentInstruct": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/LongCite-45k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/webglm-qa": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/VisionRewardDB-Video": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "zai-org/SWE-Dev-train": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/BPO": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "zai-org/LongReward-10k": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "zai-org/VisionRewardDB-Image": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "zai-org/glm-simple-evals-dataset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "zai-org/CogVLM-SFT-311K": {
        "modality": "Multimodal",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "zai-org/VisionRewardDB-Image-regression": {
        "modality": "Multimodal",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "zai-org/T1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "togethercomputer/evaluation_examples": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "ZhipuAI/MotionBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/VisionRewardDB-Image-regression": {
        "modality": "Vision",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "ZhipuAI/LongReward-10k": {
        "modality": "Language",
        "lifecircle": "Preference",
        "is_valid": true
    },
    "ZhipuAI/SWE-Dev-train": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/LongCite-45k": {
        "modality": "Language",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    },
    "ZhipuAI/T1": {
        "modality": null,
        "lifecircle": null,
        "is_valid": false
    },
    "ZhipuAI/LongBench-v2": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/glm-simple-evals-dataset": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/ComplexFuncBench": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "ZhipuAI/CC-Bench-trajectories": {
        "modality": null,
        "lifecircle": "Evaluation",
        "is_valid": true
    },
    "iic/WebShaper": {
        "modality": "Embodied",
        "lifecircle": "Fine-tuning",
        "is_valid": true
    }
}